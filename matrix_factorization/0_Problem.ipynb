{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization with NN (Problem)\n",
    "\n",
    "* Do matrix factorization by NN and to obtain weights as vectors that encode each column and row entries.\n",
    "* In this example table, we want to learn a vector representation for cat, dog, run, jump, etc., such that the inner products of (cat, run) = 0.85, (cat, jump) = 0.92, ... and so on.   \n",
    "\n",
    "|  -  | run  | jump | laugh | sleep |\n",
    "|-----|------|------|-------|-------|\n",
    "| cat | 0.85 | 0.92 | 0.95  | 1.37  | \n",
    "| dog | 0.62 | 0.77 | 0.78  | 1.14  |\n",
    "  \n",
    "* Question inspired by the lecture video [Deep Learning for Language Model](https://youtu.be/Jvigef51rqk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix M\n",
    "Here M is generated as the matrix multiplication of M1 and M2, which are not known beforehand. They naturally form a possible answer tot he factorization problem. But in general we will not get them back as our result since the answer to the factorization is not unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of M\n",
    "num_row = 2\n",
    "num_col = 4\n",
    "\n",
    "# The size of the hidden layer correspondes to the intended size of vectors to be learned  \n",
    "hidden_size = 3\n",
    "\n",
    "# Training epoch\n",
    "num_epoch = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:\n",
      " [[0.8492 0.9202 0.9473 1.3743]\n",
      " [0.6197 0.7663 0.7788 1.1388]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "M1 = np.random.random([num_row, hidden_size])\n",
    "M2 = np.random.random([hidden_size, num_col]) \n",
    "M = np.matmul(M1, M2)\n",
    "print('M:\\n', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct newtork and train\n",
    "\n",
    "* Each row of $W_1$ represents a vector (cat, dog)\n",
    "* Each column of $W_2$ represents a vector (run, jump, laugh, sleep)\n",
    "* Without having any activation functions and bias term, this is really just matrix multiplication (as intended) and does not qualify a real neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss:  9.533157\n",
      "0  loss  9.508075\n",
      "1000  loss  0.7905307\n",
      "2000  loss  0.075779274\n",
      "3000  loss  0.018555598\n",
      "4000  loss  0.0046997042\n",
      "5000  loss  0.0008214359\n",
      "6000  loss  5.5570927e-05\n",
      "7000  loss  6.3316475e-07\n",
      "8000  loss  3.6362646e-10\n",
      "9000  loss  8.686385e-13\n",
      "10000  loss  1.6342483e-13\n",
      "11000  loss  8.482104e-14\n",
      "12000  loss  1.8651747e-14\n",
      "13000  loss  5.7731597e-15\n",
      "14000  loss  3.5527137e-15\n",
      "15000  loss  3.1086245e-15\n",
      "16000  loss  1.731948e-14\n",
      "17000  loss  2.5228708e-12\n",
      "18000  loss  5.216272e-12\n",
      "19000  loss  2.5728308e-11\n",
      "W1\n",
      " [[ 0.9582  1.2214 -1.173 ]\n",
      " [ 0.2596  0.5451  0.2061]]\n",
      "W2\n",
      " [[-1.3521 -0.9959 -0.0032  1.2053]\n",
      " [ 1.7738  1.7825  1.246   1.1379]\n",
      " [ 0.0186  0.258   0.4872  0.9978]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_row])\n",
    "y = tf.placeholder(tf.float32, [None, num_col])\n",
    "\n",
    "# Matrix multiplication without bias terms\n",
    "W1 = tf.Variable(tf.random_normal([num_row, hidden_size]))\n",
    "l1 = tf.matmul(x, W1)\n",
    "W2 = tf.Variable(tf.random_normal([hidden_size, num_col]))\n",
    "pred = tf.matmul(l1, W2)\n",
    "loss = tf.reduce_mean(tf.square(pred - y)) \n",
    "\n",
    "optimize_step = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "x_data = np.eye(M.shape[0])\n",
    "y_data = M\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out_pred, out_loss = sess.run([pred, loss], feed_dict={x:x_data, y:y_data})\n",
    "    print('initial loss: ', out_loss)\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        sess.run([optimize_step, loss], feed_dict={x:x_data, y:y_data})\n",
    "        if i % 1000 == 0:\n",
    "            print(i, ' loss ', sess.run(loss, feed_dict={x:x_data, y:y_data}))\n",
    "    W1, W2 = sess.run([W1, W2])\n",
    "    print('W1\\n', W1)\n",
    "    print('W2\\n', W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE of reproduced matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M (data matrix)\n",
      " [[0.8492 0.9202 0.9473 1.3743]\n",
      " [0.6197 0.7663 0.7788 1.1388]]\n",
      "W1W2 (reproduced matrix)\n",
      " [[0.8492 0.9202 0.9473 1.3743]\n",
      " [0.6197 0.7662 0.7788 1.1388]]\n",
      "Residual:\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "mean error: 2.939346621565998e-05\n"
     ]
    }
   ],
   "source": [
    "print('M (data matrix)\\n', M)\n",
    "M_reproduced = np.matmul(W1, W2)\n",
    "print('W1W2 (reproduced matrix)\\n', M_reproduced)\n",
    "print('Residual:\\n', M - M_reproduced)\n",
    "rmse_M = np.sqrt(np.mean(np.square((M - M_reproduced))))\n",
    "print('mean error: {}'.format(rmse_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. The method appears to work, at least on the current example. It finds the factorized matrices $W_1$ and $W_2$ that give $W_1W_2 \\simeq M$. The rmse error can be lower than 1e-5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "1. Does this work on a larger matrix M?\n",
    "2. Does this work for any number of hidden layers?  \n",
    "\n",
    "See [1_Experiments.ipynb](1_Experiments.ipynb) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
